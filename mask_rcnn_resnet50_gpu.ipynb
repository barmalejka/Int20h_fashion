{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport sys\nimport collections\nimport itertools\nfrom tqdm import tqdm\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install -U torchvision\n!pip install pycocotools","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!cp -r ../input/vision-references-detection/* ./","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import torch\nimport torchvision\nfrom engine import train_one_epoch\nimport transforms as T\nimport utils\nfrom torchvision import transforms\nfrom torchvision.transforms import ToTensor\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\nfrom torchvision.models.detection import FasterRCNN\nfrom torchvision.models.detection.rpn import AnchorGenerator","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def rle_decode(mask_rle, shape):\n    shape = (shape[1], shape[0])\n    s = mask_rle.split()\n    # gets starts & lengths 1d arrays\n    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0::2], s[1::2])]\n    starts -= 1\n    # gets ends 1d array\n    ends = starts + lengths\n    # creates blank mask image 1d array\n    img = np.zeros(shape[0] * shape[1], dtype=np.uint8)\n    # sets mark pixles\n    for lo, hi in zip(starts, ends):\n        img[lo:hi] = 1\n    # reshape as a 2d mask image\n    return img.reshape(shape).T  # Needed to align to RLE direction","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert data to run-length encoding\ndef to_rle(bits):\n    rle = []\n    pos = 0\n    for bit, group in itertools.groupby(bits):\n        group_list = list(group)\n        if bit:\n            rle.extend([pos, sum(group_list)])\n        pos += len(group_list)\n    return rle","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def refine_masks(masks, labels):\n    # Compute the areas of each mask\n    areas = np.sum(masks.reshape(-1, masks.shape[-1]), axis=0)\n    # Masks are ordered from smallest to largest\n    mask_index = np.argsort(areas)\n    # One reference mask is created to be incrementally populated\n    union_mask = {k:np.zeros(masks.shape[:-1], dtype=bool) for k in np.unique(labels)}\n    # Iterate from the smallest, so smallest ones are preserved\n    for m in mask_index:\n        label = labels[m]\n        masks[:, :, m] = np.logical_and(masks[:, :, m], np.logical_not(union_mask[label]))\n        union_mask[label] = np.logical_or(masks[:, :, m], union_mask[label])\n    # Reorder masks\n    refined = list()\n    for m in range(masks.shape[-1]):\n        mask = masks[:, :, m].ravel(order='F')\n        rle = to_rle(mask)\n        label = labels[m] - 1\n        refined.append([masks[:, :, m], rle, label])\n    return refined","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Train data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/preprocessing/train_df_truncated.csv\")\ntrain = train[train['dataset']=='train'][['ImageId', 'EncodedPixels', 'Height', 'Width', 'Category']].iloc[:int(len(train)*0.2)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dataset class"},{"metadata":{"trusted":true},"cell_type":"code","source":"class FashionDataset(object):\n    def __init__(self, image_dir, df, height, width, transforms=None):\n        self.image_dir = image_dir\n        self.df = df\n        self.transforms = transforms\n        self.height = height\n        self.width = width\n        # aggregated images info\n        self.image_info = collections.defaultdict(dict)\n        \n        temp_df = self.df.groupby('ImageId')['EncodedPixels', 'Category'].agg(lambda x: list(x)).reset_index()\n        size_df = self.df.groupby('ImageId')['Height', 'Width'].mean().reset_index()\n        temp_df = temp_df.merge(size_df, on='ImageId', how='left')\n        \n        for index, row in temp_df.iterrows():#tqdm(temp_df.iterrows(), total=len(temp_df)):\n            image_id = row['ImageId']\n            image_path = os.path.join(self.image_dir, image_id)\n            self.image_info[index][\"image_id\"] = image_id\n            self.image_info[index][\"image_path\"] = image_path\n            self.image_info[index][\"labels\"] = row[\"Category\"]\n            self.image_info[index][\"width\"] = self.width\n            self.image_info[index][\"height\"] = self.height \n            self.image_info[index][\"orig_height\"] = row[\"Height\"]\n            self.image_info[index][\"orig_width\"] = row[\"Width\"]\n            self.image_info[index][\"annotations\"] = row[\"EncodedPixels\"]        \n        \n    def __getitem__(self, idx):\n        img_path = self.image_info[idx][\"image_path\"]\n        img = Image.open(img_path).convert(\"RGB\")\n        img = img.resize((self.width, self.height), resample=Image.BILINEAR)\n        \n        info = self.image_info[idx]\n        \n        mask = np.zeros((len(info['annotations']), self.width, self.height), dtype=np.uint8)\n        \n        labels = []\n        for m, (annotation, label) in enumerate(zip(info['annotations'], info['labels'])):\n            sub_mask = rle_decode(annotation, (info['orig_height'], info['orig_width']))\n            sub_mask = Image.fromarray(sub_mask)\n            sub_mask = sub_mask.resize((self.width, self.height), resample=Image.BILINEAR)\n            mask[m, :, :] = sub_mask\n            labels.append(int(label) + 1)\n\n        num_objs = len(labels)\n        boxes = []\n        new_labels = []\n        new_masks = []\n\n        for i in range(num_objs):\n            try:\n                pos = np.where(mask[i, :, :])\n                xmin = np.min(pos[1])\n                xmax = np.max(pos[1])\n                ymin = np.min(pos[0])\n                ymax = np.max(pos[0])\n                if abs(xmax - xmin) >= 20 and abs(ymax - ymin) >= 20:\n                    boxes.append([xmin, ymin, xmax, ymax])\n                    new_labels.append(labels[i])\n                    new_masks.append(mask[i, :, :])\n            except ValueError:\n                continue\n\n        if len(new_labels) == 0:\n            boxes.append([0, 0, 20, 20])\n            new_labels.append(0)\n            new_masks.append(mask[0, :, :])\n\n        nmx = np.zeros((len(new_masks), self.width, self.height), dtype=np.uint8)\n        for i, n in enumerate(new_masks):\n            nmx[i, :, :] = n\n\n        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n        labels = torch.as_tensor(new_labels, dtype=torch.int64)\n        masks = torch.as_tensor(nmx, dtype=torch.uint8)\n\n        image_id = torch.tensor([idx])\n        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n\n        target = {}\n        target[\"boxes\"] = boxes\n        target[\"labels\"] = labels\n        target[\"masks\"] = masks\n        target[\"image_id\"] = image_id\n        target[\"area\"] = area\n        target[\"iscrowd\"] = iscrowd\n\n        if self.transforms is not None:\n            img, target = self.transforms(img, target)\n\n        return img, target\n\n    def __len__(self):\n        return len(self.image_info)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Loading model"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_model_instance_segmentation(num_classes):\n    # load an instance segmentation model pre-trained pre-trained on COCO\n    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n\n    # get number of input features for the classifier\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    # replace the pre-trained head with a new one\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n\n    # get the number of input features for the mask classifier\n    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n    hidden_layer = 256\n    # replace the mask predictor with a new one\n    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n                                                       hidden_layer,\n                                                       num_classes)\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_transform(train):\n    transforms = []\n    transforms.append(T.ToTensor())\n    if train:\n        transforms.append(T.RandomHorizontalFlip(0.5))\n    return T.Compose(transforms)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Training\n(resnet50 backbone)"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_classes = 46 + 1\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.cuda.is_available()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Define dataset and dataloader"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset_train = FashionDataset(\"../input/imaterialist-fashion-2019-FGVC6/train/\",\n                               train,\n                               512, 512,\n                               transforms=get_transform(train=True))\n\ndata_loader = torch.utils.data.DataLoader(\n    dataset_train, batch_size=2, shuffle=True, num_workers=8,\n    collate_fn=utils.collate_fn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_ft = get_model_instance_segmentation(num_classes)\nmodel_ft.to(device)\n\nparams = [p for p in model_ft.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(params, lr=0.001,\n                            momentum=0.9, weight_decay=0.0005)\n\nlr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n                                               step_size=5,\n                                               gamma=0.1)\nnum_epochs = 1\n\nfor epoch in range(num_epochs):\n    train_one_epoch(model_ft, optimizer, data_loader, device, epoch, print_freq=10)\n    lr_scheduler.step()    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.save(model_ft.state_dict(), \"model_resnet.bin\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv(\"../input/preprocessing/train_df_truncated.csv\")\ntest = test[test['dataset']=='test'][['ImageId', 'EncodedPixels', 'Height', 'Width', 'Category']].reset_index().iloc[:int(len(test)*0.1)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img_path = \"../input/imaterialist-fashion-2019-FGVC6/train/\"\n\nfor param in model_ft.parameters():\n    param.requires_grad = False\n\nmodel_ft.eval()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_list = []\nmissing_count = 0\nsubmission = []\nctr = 0\n\ntt = ToTensor()\nfor i, row in tqdm(test.iterrows(), total=len(test)):\n    # loading image\n    image_id = row['ImageId']\n    img_path = os.path.join(img_path, image_id)\n    img = Image.open(img_path).convert(\"RGB\")\n    img = img.resize((512, 512), resample=Image.BILINEAR)\n    img = tt(img)\n    \n    \n    result = model_ft([img.to(device)])[0]\n    masks = np.zeros((512, 512, len(result[\"masks\"])))\n    for j, m in enumerate(result[\"masks\"]):\n        res = transforms.ToPILImage()(result[\"masks\"][j].permute(1, 2, 0).cpu().numpy())\n        res = np.asarray(res.resize((512, 512), resample=Image.BILINEAR))\n        masks[:, :, j] = (res[:, :] * 255. > 127).astype(np.uint8)\n\n    lbls = result['labels'].cpu().numpy()\n    scores = result['scores'].cpu().numpy()\n\n    best_idx = 0\n    for scr in scores:\n        if scr > 0.8:\n            best_idx += 1\n\n    if best_idx == 0:\n        sub_list.append([test.loc[i, 'ImageId'], '1 1', 23])\n        missing_count += 1\n        continue\n\n    if masks.shape[-1] > 0:\n        masks = refine_masks(masks[:, :, :best_idx], lbls[:best_idx])\n        for m, rle, label in masks:\n            sub_list.append([test.loc[i, 'ImageId'], ' '.join(list(map(str, list(rle)))), label])\n    else:\n        sub_list.append([test.loc[i, 'ImageId'], '1 1', 23])\n        missing_count += 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_df = pd.DataFrame(sub_list)\nsub_list","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}
